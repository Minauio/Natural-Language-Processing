{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080 - Mandatory assignment 2 2023\n",
    "\n",
    "## Part 1 - Exploring the NLTK tagger landscape\n",
    "\n",
    "### Exercise 1a: Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\MinaS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\MinaS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"brown\")\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "sents = brown.tagged_sents(categories='news', tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "news_train, news_val = train_test_split(sents,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1b: Most Frequent Class Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = []\n",
    "for sentence in news_train:\n",
    "    for word, tag in sentence:\n",
    "        tags.append(tag)\n",
    "\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the tagger against the gold standard is: 0.3020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MinaS\\AppData\\Local\\Temp\\ipykernel_23232\\3809515081.py:8: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f\"The accuracy of the tagger against the gold standard is: {default_tagger.evaluate(news_val):.4f}\")\n"
     ]
    }
   ],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "\n",
    "# Evaluating on validation set:\n",
    "\n",
    "print(f\"The accuracy of the tagger against the gold standard is: \n",
    "      {default_tagger.evaluate(news_val):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1c: Naive Bayes Unigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the tagger against the gold standard is: 0.8809\n"
     ]
    }
   ],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(news_train)\n",
    "\n",
    "print(f\"The accuracy of the tagger against the gold standard is:\n",
    "      {unigram_tagger.accuracy(news_val):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the universal tagset is alot higher on the unigram tagger than the default tagset as they differ as much as 0.49.\n",
    "\n",
    "### Excercise 1d: Bigram HMM Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MinaS\\AppData\\Local\\Temp\\ipykernel_23232\\3538903188.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f\"The accuracy of the tagger against the gold standard is: {hmm.evaluate(news_val):.4f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the tagger against the gold standard is: 0.9096\n"
     ]
    }
   ],
   "source": [
    "hmm = nltk.HiddenMarkovModelTagger.train(news_train)\n",
    "print(f\"The accuracy of the tagger against the gold standard is: \n",
    "      {hmm.evaluate(news_val):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is even higher than the unigram tagger.\n",
    "\n",
    "### 1e: Perceptron with greedy decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MinaS\\AppData\\Local\\Temp\\ipykernel_23232\\454315405.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(f\"The accuracy of the tagger against the gold standard is: {perc.evaluate(news_val):.4f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the tagger against the gold standard is: 0.9657\n"
     ]
    }
   ],
   "source": [
    "perc = nltk.PerceptronTagger(load=False)\n",
    "perc.train(news_train)\n",
    "print(f\"The accuracy of the tagger against the gold standard is: \n",
    "      {perc.evaluate(news_val):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm has better accuracy than all the previous.\n",
    "\n",
    "## Part 2 - Greedy LR taggers and feature engineering\n",
    "\n",
    "### Exercise 2a: Getting started with a greedy logistic regression tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class ScikitGreedyTagger(nltk.TaggerI):\n",
    "    def __init__(self, features, clf=LogisticRegression(max_iter=1000)):\n",
    "        self.features = features\n",
    "        self.classifier = clf\n",
    "        self.vectorizer = DictVectorizer()\n",
    "\n",
    "    def train(self, train_sents):\n",
    "        train_feature_sets = []\n",
    "        train_labels = []\n",
    "\n",
    "        for tagged_sent in train_sents:\n",
    "            history = []\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "        \n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                feature_set = self.features(untagged_sent, i, history)\n",
    "                train_feature_sets.append(feature_set)\n",
    "                train_labels.append(tag)\n",
    "                history.append(tag)\n",
    "        \n",
    "        x_train = self.vectorizer.fit_transform(train_feature_sets)\n",
    "        y_train = np.array(train_labels)\n",
    "        self.classifier.fit(x_train, y_train)\n",
    "    \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.features(sentence, i, history)\n",
    "            X_test = self.vectorizer.transform(featureset)\n",
    "            tags = self.classifier.predict(X_test)\n",
    "            history.append(tags)\n",
    "        return zip(sentence, history)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.920281483037496"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tagger = ScikitGreedyTagger(pos_features)\n",
    "lr_tagger.train(news_train)\n",
    "lr_tagger.accuracy(news_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this tagger compared to the DaultTagger, UnigramTagger and HiddenMarkovModelTagger is higher but lower copared to PerceptronTagger.\n",
    "\n",
    "### Exercise 2b: Adding word context features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_2b(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_prev_word\"] = \"<START 2>\"\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "    else: \n",
    "        features[\"prev_prev_word\"] = sentence[i-2]\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "\n",
    "    if i == (len(sentence)-1):\n",
    "        features[\"next word\"] = \"<END>\"\n",
    "    else: \n",
    "        features[\"next word\"] = sentence[i+1]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9289990547211427"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tagger = ScikitGreedyTagger(pos_features_2b)\n",
    "lr_tagger.train(news_train)\n",
    "lr_tagger.accuracy(news_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination that works best is the last one which added the next word and the word before the previous one as its accuracy was higher. However greedy decoding  still has the highest accuracy compared to all of the taggers.\n",
    "\n",
    "### Excercise 2c: Adding transition features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_2c_digram(sentence, i, history):\n",
    "    features = {\"curr_word\": sentence[i]}\n",
    "    if i == 0:\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "        features[\"prev_tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9165003676084444"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lr_tagger = ScikitGreedyTagger(pos_features_2c_digram)\n",
    "lr_tagger.train(news_train)\n",
    "lr_tagger.accuracy(news_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_2c_trigram(sentence, i, history):\n",
    "    features = {}\n",
    "    if i == 0:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    else: \n",
    "        features[\"prev_prev_word\"] = sentence[i-2]\n",
    "        features[\"prev_prev_tag\"] = history[i-2]\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    features[\"curr_word\"] = sentence[i]\n",
    "\n",
    "    if i == (len(sentence)-1):\n",
    "        features[\"next word\"] = \"<END>\"\n",
    "    else: \n",
    "        features[\"next word\"] = sentence[i+1]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285789307845814"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tagger = ScikitGreedyTagger(pos_features_2c_trigram)\n",
    "lr_tagger.train(news_train)\n",
    "lr_tagger.accuracy(news_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersice 2d: Even more features\n",
    "\n",
    "Try to add more features to get an even better tagger. Only the fantasy sets limits to what you may\n",
    "consider. Some ideas: Extract suffixes and prefixes from the current, previous or next word. Is the\n",
    "current word a number? Is it capitalized? Does it contain capitals? Does it contain a hyphen? etc. What is\n",
    "the best feature set you can come up with? Train and test various feature sets and select the best one.\n",
    "\n",
    "If you use sources for finding tips about good features (like articles, web pages, NLTK code, etc.) make\n",
    "references to the sources and explain what you got from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_2d_trigram_suf(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:],\n",
    "                 \"prefix(3)\": sentence[i][:3]\n",
    "                 }\n",
    "    if i == 0:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_prev_tag\"] = \"<START>\"\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    else: \n",
    "        features[\"prev_prev_word\"] = sentence[i-2]\n",
    "        features[\"prev_prev_tag\"] = history[i-2]\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    \n",
    "    features[\"curr_word\"] = sentence[i]\n",
    "\n",
    "    if i == (len(sentence)-1):\n",
    "        features[\"next word\"] = \"<END>\"\n",
    "    else:\n",
    "        features[\"next word\"] = sentence[i+1]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9633441865350278"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tagger = ScikitGreedyTagger(pos_features_2d_trigram_suf)\n",
    "lr_tagger.train(news_train)\n",
    "lr_tagger.accuracy(news_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_2d_trigram_num(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:],\n",
    "                 \"prefix(3)\": sentence[i][:3],\n",
    "                 \"Numeric\": sentence[i].isdigit()\n",
    "                 }\n",
    "    if i == 0:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_prev_tag\"] = \"<START>\"\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    else: \n",
    "        features[\"prev_prev_word\"] = sentence[i-2]\n",
    "        features[\"prev_prev_tag\"] = history[i-2]\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    \n",
    "    features[\"curr_word\"] = sentence[i]\n",
    "\n",
    "    if i == (len(sentence)-1):\n",
    "        features[\"next word\"] = \"<END>\"\n",
    "    else:\n",
    "        features[\"next word\"] = sentence[i+1]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9633441865350278"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tagger = ScikitGreedyTagger(pos_features_2d_trigram_num)\n",
    "lr_tagger.train(news_train)\n",
    "lr_tagger.accuracy(news_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_2d_trigram_cap(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:],\n",
    "                 \"prefix(3)\": sentence[i][:3],\n",
    "                 \"Capitalized\": sentence[i].isupper(),\n",
    "                 \"Any_uppercase\": any(ele.isupper() for ele in sentence[i])\n",
    "                 }\n",
    "    if i == 0:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_prev_tag\"] = \"<START>\"\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    else: \n",
    "        features[\"prev_prev_word\"] = sentence[i-2]\n",
    "        features[\"prev_prev_tag\"] = history[i-2]\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    \n",
    "    features[\"curr_word\"] = sentence[i]\n",
    "\n",
    "    if i == (len(sentence)-1):\n",
    "        features[\"next word\"] = \"<END>\"\n",
    "    else:\n",
    "        features[\"next word\"] = sentence[i+1]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9641844344081504"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tagger = ScikitGreedyTagger(pos_features_2d_trigram_cap)\n",
    "lr_tagger.train(news_train)\n",
    "lr_tagger.accuracy(news_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features_2d_trigram_hyp(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:],\n",
    "                 \"prefix(3)\": sentence[i][:3],\n",
    "                 \"Capitalized\": sentence[i].isupper(),\n",
    "                 \"Any_uppercase\": any(ele.isupper() for ele in sentence[i]),\n",
    "                 \"Hyphen\":  bool(re.search(\"-\", sentence[i]))\n",
    "                 }\n",
    "    if i == 0:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = \"<START>\"\n",
    "    elif i == 1:\n",
    "        features[\"prev_prev_word\"] = \"<START>\"\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_prev_tag\"] = \"<START>\"\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    else: \n",
    "        features[\"prev_prev_word\"] = sentence[i-2]\n",
    "        features[\"prev_prev_tag\"] = history[i-2]\n",
    "        features[\"prev_word\"] = sentence[i-1]\n",
    "        features[\"prev_tag\"] = history[i-1]\n",
    "    \n",
    "    features[\"curr_word\"] = sentence[i]\n",
    "\n",
    "    if i == (len(sentence)-1):\n",
    "        features[\"next word\"] = \"<END>\"\n",
    "    else:\n",
    "        features[\"next word\"] = sentence[i+1]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9654448062178342"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tagger = ScikitGreedyTagger(pos_features_2d_trigram_hyp)\n",
    "lr_tagger.train(news_train)\n",
    "lr_tagger.accuracy(news_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: NLTK book\n",
    "link: https://www.nltk.org/book/ch06.html\n",
    "result:\n",
    "\n",
    "There were improvment on all of the functions when adding features, but when adding the hyphen and numeric feature the accuracy dropped by 0.001.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2e: Regularization\n",
    "\n",
    "As in the previous assignment, we will study the effect of different regularization strengths now. In scikit-learn, regularization is expressed by the parameter C. A smaller C means stronger regularization. Try with \n",
    "C in [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] and see which value which yields the best result. You can also try \n",
    "additional values.\n",
    "Summarize your experiments to make clear which set of features and parameters provide the best \n",
    "results, and what the corresponding accuracy score is. Did you manage to outperform the perceptron \n",
    "tagger? If not, where do you think the bottleneck of your current tagger lies?\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "It almost outperformed the perceptron tagger with c=10. The bottleneck of current tagger lies in how big c is as it will work slower when this value is bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MinaS\\Anaconda3\\envs\\in4080_2023\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MinaS\\Anaconda3\\envs\\in4080_2023\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MinaS\\Anaconda3\\envs\\in4080_2023\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MinaS\\Anaconda3\\envs\\in4080_2023\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MinaS\\Anaconda3\\envs\\in4080_2023\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is related to c = 10.0 and is equal to accuracy = 0.9667051780275181\n"
     ]
    }
   ],
   "source": [
    "Max_accuracy = 0\n",
    "best_c = 0\n",
    "for c in C: \n",
    "    cls = ScikitGreedyTagger(features=pos_features_2d_trigram_cap,clf=LogisticRegression(C=c))\n",
    "    cls.train(news_train)\n",
    "    acc = cls.accuracy(news_val)\n",
    "    if acc > Max_accuracy:\n",
    "        Max_accuracy = acc\n",
    "        best_c = c   \n",
    "\n",
    "print(f\"The best accuracy is related to c = {best_c} and is equal to accuracy = {Max_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Training and testing on a larger corpus\n",
    "\n",
    "#### Exercise 3a: Compile the extended training and test data\n",
    "\n",
    "The NLTK book, chapter 2.1.3, lists the names of the 15 genres available in the Brown corpus. We will set \n",
    "two genres aside for testing: hobbies and adventure. For training, we will use the news training set \n",
    "prepared for the previous exercises, as well as the data from the remaining 12 genres. Prepare the \n",
    "corpus as described and store the datasets in the variables all_train, hobbies_test and \n",
    "adventure_test. We will not use news_val in this part. Make sure to use the universal tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = brown.categories()\n",
    "\n",
    "hobbies_test = brown.tagged_sents(categories='hobbies', tagset='universal')\n",
    "adventure_test = brown.tagged_sents(categories='adventure', tagset='universal')\n",
    "\n",
    "train_categories = []\n",
    "for category in categories:\n",
    "    if category not in ['hobbies', 'adventure', 'news']:\n",
    "        train_categories.append(category)\n",
    "\n",
    "train = brown.tagged_sents(categories=train_categories, tagset='universal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = train + news_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b: Evaluate the taggers\n",
    "\n",
    "Identify the most successful tagger from part 1 and the best setup from part 2. Retrain both of them on all_train and evaluate them separately on the two test genres. Report the results and discuss them briefly: Which of the two genres is “easier”? How well do the two taggers generalize to unseen genres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MinaS\\Anaconda3\\envs\\in4080_2023\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Most successful tagger from part 1 with accuracy of 0.97\n",
    "perc2 = nltk.PerceptronTagger(load=False)\n",
    "perc2.train(all_train)\n",
    "\n",
    "# Best setup from part 2 is c = 10.0 with accuracy = 0.969 \n",
    "clscap = ScikitGreedyTagger(features=pos_features_2d_trigram_cap,clf=LogisticRegression(C=10))\n",
    "clscap.train(all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9613577023498695\n",
      "0.9697297453202965\n"
     ]
    }
   ],
   "source": [
    "# Testing on hobbies set\n",
    "print(clscap.accuracy(hobbies_test))\n",
    "\n",
    "# Testing on adventure set\n",
    "print(clscap.accuracy(adventure_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9674418604651163\n",
      "0.9743012892619192\n"
     ]
    }
   ],
   "source": [
    "# Testing on hobbies set\n",
    "print(perc2.accuracy(hobbies_test))\n",
    "\n",
    "# Testing on adventure set\n",
    "print(perc2.accuracy(adventure_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genre that is easier is adventure. It seems like the perceptron tagger generalises a little bit better than the other one, but in general they both are able to generalise pretty well as they have high accuracy for the unseen test data.\n",
    "\n",
    "\n",
    "\n",
    "#### Excercise 3c: Confusion matrix\n",
    "\n",
    "The accuracy gives us a high-level overview of the performance of a tagger, but we may be interested in \n",
    "finding out more details about where the tagger makes the mistakes. The universal tagset is reasonably \n",
    "small, so we can produce a confusion matrix. Take a look at https://www.nltk.org/api/nltk.tag.api.html\n",
    "and make a confusion matrix for the results. Pick the results of one test set and one tagger. Make sure \n",
    "you understand what the rows and columns are. Which pairs of tags are most easily confounded?\n",
    "\n",
    "You can find the documentation of the tagset in the following link, but note that NLTK uses an earlier, \n",
    "slightly different version of the tagset: https://universaldependencies.org/u/pos/index.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |                             C           N           P           V       |\n",
      "     |           A     A     A     O     D     O     N     R     P     E       |\n",
      "     |           D     D     D     N     E     U     U     O     R     R       |\n",
      "     |     .     J     P     V     J     T     N     M     N     T     B     X |\n",
      "-----+-------------------------------------------------------------------------+\n",
      "   . | <9792>    .     .     .     .     .     .     .     .     .     .     . |\n",
      " ADJ |     . <6071>    6    95     .     .   297    19     .     2    69     . |\n",
      " ADP |     .     6 <9932>   31     3     3     6     .     6    80     3     . |\n",
      " ADV |     .   119    97 <3432>    5     9    53     .     .    15     6     . |\n",
      "CONJ |     .     .     3     4 <2937>    2     2     .     .     .     .     . |\n",
      " DET |     .     .    18     5     . <9469>    .     .     5     .     .     . |\n",
      "NOUN |     .   419     6    34     .     1<20427>   69     .     4   316     . |\n",
      " NUM |     .    15     .     .     .     .    35 <1375>    .     1     .     . |\n",
      "PRON |     .     .    19     .     .     9     .     . <2306>    .     .     . |\n",
      " PRT |     .     1   133    11     .     .     7     .     . <1737>    .     . |\n",
      "VERB |     .    62     8    13     .     .   486     .     .     .<12164>    . |\n",
      "   X |     .     4     .     .     .     1    57     .     .     .     1   <22>|\n",
      "-----+-------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perceptron hobbies set\n",
    "print(perc2.confusion(hobbies_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |                             C           N           P           V       |\n",
      "     |           A     A     A     O     D     O     N     R     P     E       |\n",
      "     |           D     D     D     N     E     U     U     O     R     R       |\n",
      "     |     .     J     P     V     J     T     N     M     N     T     B     X |\n",
      "-----+-------------------------------------------------------------------------+\n",
      "   . |<10929>    .     .     .     .     .     .     .     .     .     .     . |\n",
      " ADJ |     . <3040>    6   113     .     .   151     1     .     1    52     . |\n",
      " ADP |     .     5 <6916>   41     3     .    13     .     3    83     5     . |\n",
      " ADV |     .   114    48 <3598>    6    16    59     1     .    25    12     . |\n",
      "CONJ |     .     .     2     1 <2155>   12     2     .     .     .     1     . |\n",
      " DET |     .     .    20    13     2 <8079>    3     2    36     .     .     . |\n",
      "NOUN |     .   159     1    35     .     .<13013>   16     1     8   121     . |\n",
      " NUM |     .     1     .     .     .     .     6  <459>    .     .     .     . |\n",
      "PRON |     .     .     8     2     .    85     9     . <5099>    1     1     . |\n",
      " PRT |     .     4   132    19     .     .    72     .     1 <2197>   11     . |\n",
      "VERB |     .    33    11    11     .     1   139     .     .     5<12074>    . |\n",
      "   X |     .     .     .     .     .     .    34     .     .     .     3    <1>|\n",
      "-----+-------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perceptron Adventure set\n",
    "print(perc2.confusion(adventure_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from itertools import chain\n",
    "\n",
    "def reporting_accuracy(report, tagger, test_set):\n",
    "    # Get predictions\n",
    "    predicted_sents = tagger.tag_sents([[word for word, _ in sent] for sent in test_set])\n",
    "\n",
    "    # Extract tags from test data and predictions\n",
    "    true_tags = list(chain.from_iterable([tag for _, tag in sent] for sent in test_set))\n",
    "    predicted_tags = list(chain.from_iterable([tag for _, tag in sent] for sent in predicted_sents))\n",
    "\n",
    "    # Print classification report\n",
    "    print(report(true_tags, predicted_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9792     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0  5817     6   195     0     3   373     1     0     2   162     0]\n",
      " [    0     7  9795    58     4    10     5     0     8   180     2     1]\n",
      " [    0   136   121  3405     4     8    40     0     0    17     5     0]\n",
      " [    0     0     6     2  2938     2     0     0     0     0     0     0]\n",
      " [    0     0     9     6     0  9478     0     0     4     0     0     0]\n",
      " [   29   350    13    63     1     5 20283    48     4    13   452    15]\n",
      " [    0     4     0     0     0     0    17  1405     0     0     0     0]\n",
      " [    0     0    15     0     0    13     0     0  2306     0     0     0]\n",
      " [    0     1    83    12     0     0     6     0     0  1787     0     0]\n",
      " [    0    76    20    17     0     2   480     1     0     2 12135     0]\n",
      " [    0     4     0     0     0     2    55     0     0     1     1    22]]\n"
     ]
    }
   ],
   "source": [
    "# log tagger hobbies set\n",
    "reporting_accuracy(confusion_matrix, clscap, hobbies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10929     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0  2927     6   157     0     1   163     2     1     2   105     0]\n",
      " [    0     8  6808    61     3     7     8     0     7   159     8     0]\n",
      " [    0   113    48  3599    13    14    39     0     0    35    18     0]\n",
      " [    0     0     5     0  2155    13     0     0     0     0     0     0]\n",
      " [    0     0    14    22     2  8076     1     2    37     0     1     0]\n",
      " [    0   188     1    41     0     3 12906    19     4    22   163     7]\n",
      " [    0     0     0     0     0     0     6   459     0     0     1     0]\n",
      " [    0     1     9     0     0    90     7     0  5093     5     0     0]\n",
      " [    0     4   109    21     0     0    44     0     1  2247    10     0]\n",
      " [    0    51    10    13     0     0   150     0     1     6 12043     0]\n",
      " [    0     0     0     0     0     0    33     0     1     2     1     1]]\n"
     ]
    }
   ],
   "source": [
    "# log tagger adventure set\n",
    "reporting_accuracy(confusion_matrix, clscap, adventure_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 3d: Precision, recall and f-measure\n",
    "\n",
    "Finding hints on the NLTK web page linked above, calculate the precision, recall and f-measure for each \n",
    "tag and display the results in a table.\n",
    "\n",
    "Also calculate the macro precision, macro recall and macro f-measure across all tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Hobbies\n",
      " Tag | Prec.  | Recall | F-measure\n",
      "-----+--------+--------+-----------\n",
      "   . | 1.0000 | 1.0000 | 1.0000\n",
      " ADJ | 0.9089 | 0.9290 | 0.9188\n",
      " ADP | 0.9720 | 0.9872 | 0.9796\n",
      " ADV | 0.9521 | 0.9194 | 0.9355\n",
      "CONJ | 0.9973 | 0.9969 | 0.9971\n",
      " DET | 0.9978 | 0.9972 | 0.9975\n",
      "NOUN | 0.9566 | 0.9611 | 0.9589\n",
      " NUM | 0.9442 | 0.9734 | 0.9586\n",
      "PRON | 0.9957 | 0.9897 | 0.9927\n",
      " PRT | 0.9478 | 0.9227 | 0.9351\n",
      "VERB | 0.9696 | 0.9553 | 0.9624\n",
      "   X | 0.9231 | 0.2824 | 0.4324\n",
      "\n",
      "Perceptron Advantures\n",
      " Tag | Prec.  | Recall | F-measure\n",
      "-----+--------+--------+-----------\n",
      "   . | 1.0000 | 1.0000 | 1.0000\n",
      " ADJ | 0.9051 | 0.9040 | 0.9045\n",
      " ADP | 0.9690 | 0.9769 | 0.9730\n",
      " ADV | 0.9377 | 0.9276 | 0.9326\n",
      "CONJ | 0.9922 | 0.9913 | 0.9917\n",
      " DET | 0.9866 | 0.9913 | 0.9889\n",
      "NOUN | 0.9648 | 0.9746 | 0.9697\n",
      " NUM | 0.9586 | 0.9936 | 0.9758\n",
      "PRON | 0.9926 | 0.9800 | 0.9863\n",
      " PRT | 0.9445 | 0.9085 | 0.9261\n",
      "VERB | 0.9838 | 0.9832 | 0.9835\n",
      "   X | 1.0000 | 0.0526 | 0.1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron Hobbies\")\n",
    "print(perc2.evaluate_per_tag(hobbies_test))\n",
    "print(\"Perceptron Advantures\")\n",
    "print(perc2.evaluate_per_tag(adventure_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       1.00      1.00      1.00     10929\n",
      "         ADJ       0.89      0.87      0.88      3364\n",
      "         ADP       0.97      0.97      0.97      7069\n",
      "         ADV       0.93      0.92      0.92      3879\n",
      "        CONJ       0.99      0.99      0.99      2173\n",
      "         DET       0.98      0.99      0.99      8155\n",
      "        NOUN       0.96      0.97      0.97     13354\n",
      "         NUM       0.95      0.99      0.97       466\n",
      "        PRON       0.99      0.98      0.98      5205\n",
      "         PRT       0.91      0.91      0.91      2436\n",
      "        VERB       0.98      0.98      0.98     12274\n",
      "           X       0.25      0.03      0.05        38\n",
      "\n",
      "    accuracy                           0.97     69342\n",
      "   macro avg       0.90      0.88      0.88     69342\n",
      "weighted avg       0.97      0.97      0.97     69342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reporting_accuracy(classification_report, clscap, adventure_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       1.00      1.00      1.00      9792\n",
      "         ADJ       0.91      0.89      0.90      6559\n",
      "         ADP       0.97      0.98      0.97     10070\n",
      "         ADV       0.92      0.90      0.91      3736\n",
      "        CONJ       1.00      1.00      1.00      2948\n",
      "         DET       1.00      1.00      1.00      9497\n",
      "        NOUN       0.95      0.96      0.95     21276\n",
      "         NUM       0.96      0.98      0.97      1426\n",
      "        PRON       0.99      0.99      0.99      2334\n",
      "         PRT       0.92      0.93      0.92      1889\n",
      "        VERB       0.96      0.95      0.95     12733\n",
      "           X       0.50      0.26      0.34        85\n",
      "\n",
      "    accuracy                           0.96     82345\n",
      "   macro avg       0.92      0.90      0.91     82345\n",
      "weighted avg       0.96      0.96      0.96     82345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reporting_accuracy(classification_report, clscap, hobbies_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 3e: Error analysis\n",
    "\n",
    "Sometimes, it makes sense to inspect the output of a machine learning model more thoroughly. Find five \n",
    "sentences in the test set where at least one token is misclassified and display these sentences in the \n",
    "following format, with both the predicted and gold tags.\n",
    "\n",
    "Identify the words that are tagged differently. Comment on each of the differences. Would you say that \n",
    "the predicted tag is wrong? Or is there a genuine ambiguity such that both answers are defendable? Or is \n",
    "even the gold tag wrong?\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "For the perceptron:\n",
    "\n",
    "word------- predicted ----------        gold:\n",
    "\n",
    "1. her-------                 DET----------       PRON   \n",
    "\n",
    "A determiner is a word that introduces a noun and provides information about the oun, while pronoun is a word used to replace a noun to avoid repetition.Thus there could be ambiguity between these. \"Her\" could be a possessive determiner, but it has to be used before nouns or noun phrases. Since here it was not used before noun phrases it is not a determiner.\n",
    "\n",
    "2. fickle-------              VERB ----------     ADJ  \n",
    "\n",
    "There could not be a ambiguity here.\n",
    "\n",
    "3. sleep-------               VERB----------      NOUN\n",
    "\n",
    "sleep could be both verb and noun, but here it was wrongly predicted as verb.\n",
    "\n",
    "4. her-------                 DET----------       PRON    \n",
    "\n",
    "Here we have the same issue er (1). Thus the tag has been predicted wrong.\n",
    "\n",
    "4. much-------                ADJ----------       ADV \n",
    "\n",
    "Adjectives are words that describe the qualities or state of being of nouns. Adverbs are wordds that describes a verb. eks. \"He sings loudly\", here \"loudly\" is an adverb.adverbs can also describe a whole sentence eks. \"Fortuenately, i had brougt an umbrella.\", here \"Fortunately\" is adverb. Thus the tag was predicted wrongly.\n",
    "\n",
    "5. stubborn-------            NOUN----------      ADJ \n",
    "\n",
    "wrongly predicted tag.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporting_accuracy_sents(tagger, test_set):\n",
    "    \n",
    "    \n",
    "    # Get predictions\n",
    "    predicted_sents = tagger.tag_sents([[word for word, _ in sent] for sent in test_set])\n",
    "    sents = list([word for word,_ in sent] for sent in test_set)\n",
    "    true_tags = list([tag for _, tag in sent] for sent in test_set)\n",
    "    predicted_tags = list([tag for _, tag in sent] for sent in predicted_sents)\n",
    "    \n",
    "    s = 0\n",
    "    for i in range(len(test_set)):\n",
    "        if predicted_tags[i] != true_tags[i] and s < 5:\n",
    "            s +=1\n",
    "            print(\"\\n \\n\")\n",
    "            print(\"Token\"+\" \"*15 + \"pred\" + \" \"*5 + \"gold\")\n",
    "            print(\"-\"*35)\n",
    "            for j in range(len(predicted_tags[i])):\n",
    "                word_width = 20\n",
    "                tag_width = 10\n",
    "\n",
    "                # Format and print each field with equal width\n",
    "                formatted_output = \"{:<{}}{:<{}}{:<{}}\".format(sents[i][j], word_width, predicted_tags[i][j], tag_width, true_tags[i][j], tag_width)\n",
    "                print(formatted_output)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "\n",
      "Token               pred     gold\n",
      "-----------------------------------\n",
      "He                  PRON      PRON      \n",
      "was                 VERB      VERB      \n",
      "well                ADV       ADV       \n",
      "rid                 ADJ       ADJ       \n",
      "of                  ADP       ADP       \n",
      "her                 DET       PRON      \n",
      ".                   .         .         \n",
      "\n",
      " \n",
      "\n",
      "Token               pred     gold\n",
      "-----------------------------------\n",
      "He                  PRON      PRON      \n",
      "certainly           ADV       ADV       \n",
      "didn't              VERB      VERB      \n",
      "want                VERB      VERB      \n",
      "a                   DET       DET       \n",
      "wife                NOUN      NOUN      \n",
      "who                 PRON      PRON      \n",
      "was                 VERB      VERB      \n",
      "fickle              VERB      ADJ       \n",
      "as                  ADP       ADP       \n",
      "Ann                 NOUN      NOUN      \n",
      ".                   .         .         \n",
      "\n",
      " \n",
      "\n",
      "Token               pred     gold\n",
      "-----------------------------------\n",
      "Sometimes           ADV       ADV       \n",
      "he                  PRON      PRON      \n",
      "woke                VERB      VERB      \n",
      "up                  PRT       PRT       \n",
      "in                  ADP       ADP       \n",
      "the                 DET       DET       \n",
      "middle              NOUN      NOUN      \n",
      "of                  ADP       ADP       \n",
      "the                 DET       DET       \n",
      "night               NOUN      NOUN      \n",
      "thinking            NOUN      VERB      \n",
      "of                  ADP       ADP       \n",
      "Ann                 NOUN      NOUN      \n",
      ",                   .         .         \n",
      "and                 CONJ      CONJ      \n",
      "then                ADV       ADV       \n",
      "could               VERB      VERB      \n",
      "not                 ADV       ADV       \n",
      "get                 VERB      VERB      \n",
      "back                ADV       ADV       \n",
      "to                  ADP       ADP       \n",
      "sleep               VERB      NOUN      \n",
      ".                   .         .         \n",
      "\n",
      " \n",
      "\n",
      "Token               pred     gold\n",
      "-----------------------------------\n",
      "His                 DET       DET       \n",
      "plans               NOUN      NOUN      \n",
      "and                 CONJ      CONJ      \n",
      "dreams              NOUN      NOUN      \n",
      "had                 VERB      VERB      \n",
      "revolved            VERB      VERB      \n",
      "around              ADP       ADP       \n",
      "her                 DET       PRON      \n",
      "so                  ADV       ADV       \n",
      "much                ADJ       ADV       \n",
      "and                 CONJ      CONJ      \n",
      "for                 ADP       ADP       \n",
      "so                  ADV       ADV       \n",
      "long                ADJ       ADJ       \n",
      "that                ADP       ADP       \n",
      "now                 ADV       ADV       \n",
      "he                  PRON      PRON      \n",
      "felt                VERB      VERB      \n",
      "as                  ADP       ADP       \n",
      "if                  ADP       ADP       \n",
      "he                  PRON      PRON      \n",
      "had                 VERB      VERB      \n",
      "nothing             NOUN      NOUN      \n",
      ".                   .         .         \n",
      "\n",
      " \n",
      "\n",
      "Token               pred     gold\n",
      "-----------------------------------\n",
      "The                 DET       DET       \n",
      "easiest             ADJ       ADJ       \n",
      "thing               NOUN      NOUN      \n",
      "would               VERB      VERB      \n",
      "be                  VERB      VERB      \n",
      "to                  PRT       PRT       \n",
      "sell                VERB      VERB      \n",
      "out                 PRT       PRT       \n",
      "to                  ADP       ADP       \n",
      "Al                  NOUN      NOUN      \n",
      "Budd                NOUN      NOUN      \n",
      "and                 CONJ      CONJ      \n",
      "leave               VERB      VERB      \n",
      "the                 DET       DET       \n",
      "country             NOUN      NOUN      \n",
      ",                   .         .         \n",
      "but                 CONJ      CONJ      \n",
      "there               PRT       PRT       \n",
      "was                 VERB      VERB      \n",
      "a                   DET       DET       \n",
      "stubborn            NOUN      ADJ       \n",
      "streak              NOUN      NOUN      \n",
      "in                  ADP       ADP       \n",
      "him                 PRON      PRON      \n",
      "that                ADP       DET       \n",
      "wouldn't            VERB      VERB      \n",
      "allow               VERB      VERB      \n",
      "it                  PRON      PRON      \n",
      ".                   .         .         \n"
     ]
    }
   ],
   "source": [
    "reporting_accuracy_sents(perc, adventure_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
